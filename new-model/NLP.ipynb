{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from detect_outlier import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.metrics import Metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized = read_data()\n",
    "df_normalized.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(label):\n",
    "    if label == 0:\n",
    "        return np.random.randint(300, 580)\n",
    "    if label == 1:\n",
    "        return np.random.randint(580, 670)\n",
    "    if label == 2:\n",
    "        return np.random.randint(670, 740)\n",
    "    if label == 3:\n",
    "        return np.random.randint(740, 800)\n",
    "    if label == 4:\n",
    "        return np.random.randint(800, 850)\n",
    "def labeling(scores):\n",
    "    label = []\n",
    "    for score in scores:\n",
    "        if  score < 580:\n",
    "            label.append(0)\n",
    "        elif score >=580 and score < 670:\n",
    "            label.append(1)\n",
    "        elif score >= 670 and score < 740:\n",
    "            label.append(2)\n",
    "        elif score >= 740 and score < 800:\n",
    "            label.append(3)\n",
    "        elif score >= 800:\n",
    "            label.append(4)\n",
    "    return np.array(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Thử thêm label là điểm ở giữa\n",
    "- Lấy đầu ra của giải thuật di truyền để làm input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df_normalized[\n",
    "    [\n",
    "        \"totalAsset\",\n",
    "        \"averageTotalAsset\",\n",
    "        \"frequencyMountOfTransaction\",\n",
    "        \"borrow_per_balance\",\n",
    "        \"deposit_per_asset\",\n",
    "        \"borrow_per_deposit\",\n",
    "        \"totalValueOfLiquidation\",\n",
    "        \"numberOfLiquidation\",\n",
    "        \"frequencyOfTransaction\",\n",
    "        \"frequencyOfDappTransactions\",\n",
    "        \"numberOfInteractedDapps\",\n",
    "        \"typesOfInteractedDapps\",\n",
    "        \"numberOfReputableDapps\",\n",
    "        \"age\",\n",
    "    ]\n",
    "].values)\n",
    "main_y = df_normalized[\"1st_label\"].values\n",
    "sub_y = df_normalized[\"2nd_label\"].values\n",
    "\n",
    "X_train, X_test, main_y_train, main_y_test, sub_y_train, sub_y_test = train_test_split(\n",
    "    X, main_y, sub_y, stratify=main_y, test_size=0.2, random_state=42\n",
    ")\n",
    "y_array2d = np.stack((main_y_train, sub_y_train), axis=1)\n",
    "y_test_array2d = np.stack((main_y_test, sub_y_test), axis=1)\n",
    "# Chuẩn bị dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(14,)),  \n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSparseCategoricalCrossentropy(tf.keras.losses.Loss):\n",
    "    def __init__(\n",
    "        self,\n",
    "        from_logits=False,\n",
    "        reduction=tf.keras.losses.Reduction.AUTO,\n",
    "        name=\"custom_sparse_categorical_crossentropy\",\n",
    "    ):\n",
    "        super().__init__(reduction=reduction, name=name)\n",
    "        self.from_logits = from_logits\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Calculate the standard sparse categorical crossentropy loss\n",
    "        first_label = tf.cast(y_true[:, 0], tf.int32)\n",
    "        sencond_label = tf.cast(y_true[:, 1], tf.int32)\n",
    "        first_scce_loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            first_label, y_pred, from_logits=self.from_logits\n",
    "        )\n",
    "        sencond_scce_loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            sencond_label, y_pred, from_logits=self.from_logits\n",
    "        )\n",
    "\n",
    "        return first_scce_loss\n",
    "class CustomAccuracy(Metric):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomAccuracy, self).__init__(**kwargs)\n",
    "        self.correct = self.add_weight(\"correct\", initializer=\"zeros\")\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.argmax(y_pred, axis=1)\n",
    "        # Cast y_true to the same dtype as y_pred\n",
    "        first_label = tf.cast(y_true[:, 0], y_pred.dtype)\n",
    "        second_label = tf.cast(y_true[:, 1], y_pred.dtype)\n",
    "        # Check the equality of the prediction and truth\n",
    "        values = tf.cast(tf.logical_or(tf.equal(y_pred, first_label), tf.equal(y_pred, second_label)), 'float32')\n",
    "        self.correct.assign_add(tf.reduce_sum(values))\n",
    "        self.total.assign_add(tf.cast(tf.size(first_label), 'float32'))\n",
    "\n",
    "    def result(self):\n",
    "        return self.correct / self.total\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.correct.assign(0)\n",
    "        self.total.assign(0)\n",
    "\n",
    "# Example usage of the custom loss function\n",
    "custom_accuracy = CustomAccuracy()\n",
    "custom_loss = CustomSparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2859/2859 [==============================] - 3s 940us/step - loss: 2.1311 - custom_accuracy_10: 0.8883\n",
      "Epoch 2/15\n",
      "2859/2859 [==============================] - 3s 896us/step - loss: 0.5833 - custom_accuracy_10: 0.9132\n",
      "Epoch 3/15\n",
      "2859/2859 [==============================] - 2s 867us/step - loss: 0.3372 - custom_accuracy_10: 0.9212\n",
      "Epoch 4/15\n",
      "2859/2859 [==============================] - 3s 898us/step - loss: 0.2496 - custom_accuracy_10: 0.9264\n",
      "Epoch 5/15\n",
      "2859/2859 [==============================] - 3s 886us/step - loss: 0.2156 - custom_accuracy_10: 0.9294\n",
      "Epoch 6/15\n",
      "2859/2859 [==============================] - 3s 964us/step - loss: 0.1976 - custom_accuracy_10: 0.9335\n",
      "Epoch 7/15\n",
      "2859/2859 [==============================] - 3s 1ms/step - loss: 0.1838 - custom_accuracy_10: 0.9374\n",
      "Epoch 8/15\n",
      "2859/2859 [==============================] - 3s 883us/step - loss: 0.1785 - custom_accuracy_10: 0.9382\n",
      "Epoch 9/15\n",
      "2859/2859 [==============================] - 3s 897us/step - loss: 0.1718 - custom_accuracy_10: 0.9401\n",
      "Epoch 10/15\n",
      "2859/2859 [==============================] - 3s 925us/step - loss: 0.1686 - custom_accuracy_10: 0.9401\n",
      "Epoch 11/15\n",
      "2859/2859 [==============================] - 2s 811us/step - loss: 0.1642 - custom_accuracy_10: 0.9422\n",
      "Epoch 12/15\n",
      "2859/2859 [==============================] - 3s 944us/step - loss: 0.1622 - custom_accuracy_10: 0.9424\n",
      "Epoch 13/15\n",
      "2859/2859 [==============================] - 3s 879us/step - loss: 0.1613 - custom_accuracy_10: 0.9418\n",
      "Epoch 14/15\n",
      "2859/2859 [==============================] - 3s 1ms/step - loss: 0.1631 - custom_accuracy_10: 0.9412\n",
      "Epoch 15/15\n",
      "2859/2859 [==============================] - 3s 992us/step - loss: 0.1571 - custom_accuracy_10: 0.9441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18b9055be90>"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "# model.fit(X_train, main_y_train, epochs=15, batch_size=32)\n",
    "\n",
    "model.compile(optimizer='adam', loss=custom_loss, metrics=[custom_accuracy])\n",
    "model.fit(X_train, y_array2d , epochs=15, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715/715 [==============================] - 1s 840us/step\n",
      "Dự đoán: [2 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# X_new = np.expand_dims(X_test, axis=0)\n",
    "predictions = model.predict(X_test)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "print(\"Dự đoán:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9498863040055974, 21603, 119)\n"
     ]
    }
   ],
   "source": [
    "def new_accuracy(y_train, sub_y_train, y_pred):\n",
    "    y_train = np.array(y_train)\n",
    "    sub_y_train = np.array(sub_y_train)\n",
    "    y_pred = np.array(y_pred)\n",
    "    main_label_count = np.sum(y_pred == y_train)\n",
    "    condition = np.logical_or(y_pred == sub_y_train, y_pred == y_train)\n",
    "    count = np.sum(condition)\n",
    "    accuracy = count / len(y_train)\n",
    "    return accuracy, main_label_count, count - main_label_count\n",
    "print(new_accuracy(y_test_array2d[:, 0], y_test_array2d[:, 1], predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_array2d[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 91, 1: 19181, 2: 3079, 3: 216, 4: 301}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Sử dụng numpy.unique để lấy các nhãn duy nhất và số lần xuất hiện của chúng\n",
    "unique_labels, counts = np.unique(predicted_labels, return_counts=True)\n",
    "\n",
    "# Kết hợp kết quả vào một dictionary hoặc một mảng hai chiều để dễ dàng truy cập\n",
    "label_count_dict = dict(zip(unique_labels, counts))\n",
    "print(label_count_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
